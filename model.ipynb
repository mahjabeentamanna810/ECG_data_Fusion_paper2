{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-12 18:26:46.248271: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-12 18:26:46.450435: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-12 18:26:46.450505: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-12 18:26:46.483433: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-06-12 18:26:46.539170: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-12 18:26:49.796858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 8237 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:1a:00.0, compute capability: 8.6\n",
      "2024-06-12 18:26:49.798496: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 8168 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:68:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)        [(None, 128, 128, 3)]        0         []                            \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 64, 64, 64)           9472      ['input_1[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2  (None, 32, 32, 64)           0         ['conv2d[0][0]']              \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 32, 32, 64)           256       ['max_pooling2d[0][0]']       \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 64)           36928     ['batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 32, 32, 64)           256       ['conv2d_1[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu (LeakyReLU)     (None, 32, 32, 64)           0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " global_average_pooling2d (  (None, 64)                   0         ['leaky_re_lu[0][0]']         \n",
      " GlobalAveragePooling2D)                                                                          \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 4)                    260       ['global_average_pooling2d[0][\n",
      "                                                                    0]']                          \n",
      "                                                                                                  \n",
      " dense_1 (Dense)             (None, 64)                   320       ['dense[0][0]']               \n",
      "                                                                                                  \n",
      " reshape (Reshape)           (None, 1, 1, 64)             0         ['dense_1[0][0]']             \n",
      "                                                                                                  \n",
      " multiply (Multiply)         (None, 32, 32, 64)           0         ['leaky_re_lu[0][0]',         \n",
      "                                                                     'reshape[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)           (None, 32, 32, 64)           36928     ['multiply[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 32, 32, 64)           256       ['conv2d_2[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 32, 32, 64)           0         ['batch_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 32, 32, 64)           0         ['leaky_re_lu_1[0][0]',       \n",
      "                                                                     'batch_normalization[0][0]'] \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)           (None, 32, 32, 64)           36928     ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 32, 32, 64)           256       ['conv2d_3[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 32, 32, 64)           0         ['batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " global_average_pooling2d_1  (None, 64)                   0         ['leaky_re_lu_2[0][0]']       \n",
      "  (GlobalAveragePooling2D)                                                                        \n",
      "                                                                                                  \n",
      " dense_2 (Dense)             (None, 4)                    260       ['global_average_pooling2d_1[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dense_3 (Dense)             (None, 64)                   320       ['dense_2[0][0]']             \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)         (None, 1, 1, 64)             0         ['dense_3[0][0]']             \n",
      "                                                                                                  \n",
      " multiply_1 (Multiply)       (None, 32, 32, 64)           0         ['leaky_re_lu_2[0][0]',       \n",
      "                                                                     'reshape_1[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)           (None, 32, 32, 64)           36928     ['multiply_1[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 32, 32, 64)           256       ['conv2d_4[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 32, 32, 64)           0         ['batch_normalization_4[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " add_1 (Add)                 (None, 32, 32, 64)           0         ['leaky_re_lu_3[0][0]',       \n",
      "                                                                     'add[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)           (None, 16, 16, 128)          73856     ['add_1[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_5 (Bat  (None, 16, 16, 128)          512       ['conv2d_5[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 16, 16, 128)          0         ['batch_normalization_5[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " global_average_pooling2d_2  (None, 128)                  0         ['leaky_re_lu_4[0][0]']       \n",
      "  (GlobalAveragePooling2D)                                                                        \n",
      "                                                                                                  \n",
      " dense_4 (Dense)             (None, 8)                    1032      ['global_average_pooling2d_2[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dense_5 (Dense)             (None, 128)                  1152      ['dense_4[0][0]']             \n",
      "                                                                                                  \n",
      " reshape_2 (Reshape)         (None, 1, 1, 128)            0         ['dense_5[0][0]']             \n",
      "                                                                                                  \n",
      " multiply_2 (Multiply)       (None, 16, 16, 128)          0         ['leaky_re_lu_4[0][0]',       \n",
      "                                                                     'reshape_2[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)           (None, 16, 16, 128)          147584    ['multiply_2[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 16, 16, 128)          512       ['conv2d_6[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 16, 16, 128)          0         ['batch_normalization_6[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)           (None, 16, 16, 128)          8320      ['add_1[0][0]']               \n",
      "                                                                                                  \n",
      " add_2 (Add)                 (None, 16, 16, 128)          0         ['leaky_re_lu_5[0][0]',       \n",
      "                                                                     'conv2d_7[0][0]']            \n",
      "                                                                                                  \n",
      " conv2d_8 (Conv2D)           (None, 16, 16, 128)          147584    ['add_2[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 16, 16, 128)          512       ['conv2d_8[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 16, 16, 128)          0         ['batch_normalization_7[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " global_average_pooling2d_3  (None, 128)                  0         ['leaky_re_lu_6[0][0]']       \n",
      "  (GlobalAveragePooling2D)                                                                        \n",
      "                                                                                                  \n",
      " dense_6 (Dense)             (None, 8)                    1032      ['global_average_pooling2d_3[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dense_7 (Dense)             (None, 128)                  1152      ['dense_6[0][0]']             \n",
      "                                                                                                  \n",
      " reshape_3 (Reshape)         (None, 1, 1, 128)            0         ['dense_7[0][0]']             \n",
      "                                                                                                  \n",
      " multiply_3 (Multiply)       (None, 16, 16, 128)          0         ['leaky_re_lu_6[0][0]',       \n",
      "                                                                     'reshape_3[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_9 (Conv2D)           (None, 16, 16, 128)          147584    ['multiply_3[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 16, 16, 128)          512       ['conv2d_9[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_7 (LeakyReLU)   (None, 16, 16, 128)          0         ['batch_normalization_8[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " add_3 (Add)                 (None, 16, 16, 128)          0         ['leaky_re_lu_7[0][0]',       \n",
      "                                                                     'add_2[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_10 (Conv2D)          (None, 8, 8, 256)            295168    ['add_3[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_9 (Bat  (None, 8, 8, 256)            1024      ['conv2d_10[0][0]']           \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " leaky_re_lu_8 (LeakyReLU)   (None, 8, 8, 256)            0         ['batch_normalization_9[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " global_average_pooling2d_4  (None, 256)                  0         ['leaky_re_lu_8[0][0]']       \n",
      "  (GlobalAveragePooling2D)                                                                        \n",
      "                                                                                                  \n",
      " dense_8 (Dense)             (None, 16)                   4112      ['global_average_pooling2d_4[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dense_9 (Dense)             (None, 256)                  4352      ['dense_8[0][0]']             \n",
      "                                                                                                  \n",
      " reshape_4 (Reshape)         (None, 1, 1, 256)            0         ['dense_9[0][0]']             \n",
      "                                                                                                  \n",
      " multiply_4 (Multiply)       (None, 8, 8, 256)            0         ['leaky_re_lu_8[0][0]',       \n",
      "                                                                     'reshape_4[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_11 (Conv2D)          (None, 8, 8, 256)            590080    ['multiply_4[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_10 (Ba  (None, 8, 8, 256)            1024      ['conv2d_11[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_9 (LeakyReLU)   (None, 8, 8, 256)            0         ['batch_normalization_10[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_12 (Conv2D)          (None, 8, 8, 256)            33024     ['add_3[0][0]']               \n",
      "                                                                                                  \n",
      " add_4 (Add)                 (None, 8, 8, 256)            0         ['leaky_re_lu_9[0][0]',       \n",
      "                                                                     'conv2d_12[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_13 (Conv2D)          (None, 8, 8, 256)            590080    ['add_4[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_11 (Ba  (None, 8, 8, 256)            1024      ['conv2d_13[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_10 (LeakyReLU)  (None, 8, 8, 256)            0         ['batch_normalization_11[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " global_average_pooling2d_5  (None, 256)                  0         ['leaky_re_lu_10[0][0]']      \n",
      "  (GlobalAveragePooling2D)                                                                        \n",
      "                                                                                                  \n",
      " dense_10 (Dense)            (None, 16)                   4112      ['global_average_pooling2d_5[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dense_11 (Dense)            (None, 256)                  4352      ['dense_10[0][0]']            \n",
      "                                                                                                  \n",
      " reshape_5 (Reshape)         (None, 1, 1, 256)            0         ['dense_11[0][0]']            \n",
      "                                                                                                  \n",
      " multiply_5 (Multiply)       (None, 8, 8, 256)            0         ['leaky_re_lu_10[0][0]',      \n",
      "                                                                     'reshape_5[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_14 (Conv2D)          (None, 8, 8, 256)            590080    ['multiply_5[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_12 (Ba  (None, 8, 8, 256)            1024      ['conv2d_14[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_11 (LeakyReLU)  (None, 8, 8, 256)            0         ['batch_normalization_12[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_5 (Add)                 (None, 8, 8, 256)            0         ['leaky_re_lu_11[0][0]',      \n",
      "                                                                     'add_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_15 (Conv2D)          (None, 4, 4, 512)            1180160   ['add_5[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_13 (Ba  (None, 4, 4, 512)            2048      ['conv2d_15[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_12 (LeakyReLU)  (None, 4, 4, 512)            0         ['batch_normalization_13[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " global_average_pooling2d_6  (None, 512)                  0         ['leaky_re_lu_12[0][0]']      \n",
      "  (GlobalAveragePooling2D)                                                                        \n",
      "                                                                                                  \n",
      " dense_12 (Dense)            (None, 32)                   16416     ['global_average_pooling2d_6[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dense_13 (Dense)            (None, 512)                  16896     ['dense_12[0][0]']            \n",
      "                                                                                                  \n",
      " reshape_6 (Reshape)         (None, 1, 1, 512)            0         ['dense_13[0][0]']            \n",
      "                                                                                                  \n",
      " multiply_6 (Multiply)       (None, 4, 4, 512)            0         ['leaky_re_lu_12[0][0]',      \n",
      "                                                                     'reshape_6[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_16 (Conv2D)          (None, 4, 4, 512)            2359808   ['multiply_6[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_14 (Ba  (None, 4, 4, 512)            2048      ['conv2d_16[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_13 (LeakyReLU)  (None, 4, 4, 512)            0         ['batch_normalization_14[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " conv2d_17 (Conv2D)          (None, 4, 4, 512)            131584    ['add_5[0][0]']               \n",
      "                                                                                                  \n",
      " add_6 (Add)                 (None, 4, 4, 512)            0         ['leaky_re_lu_13[0][0]',      \n",
      "                                                                     'conv2d_17[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_18 (Conv2D)          (None, 4, 4, 512)            2359808   ['add_6[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_15 (Ba  (None, 4, 4, 512)            2048      ['conv2d_18[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_14 (LeakyReLU)  (None, 4, 4, 512)            0         ['batch_normalization_15[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " global_average_pooling2d_7  (None, 512)                  0         ['leaky_re_lu_14[0][0]']      \n",
      "  (GlobalAveragePooling2D)                                                                        \n",
      "                                                                                                  \n",
      " dense_14 (Dense)            (None, 32)                   16416     ['global_average_pooling2d_7[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dense_15 (Dense)            (None, 512)                  16896     ['dense_14[0][0]']            \n",
      "                                                                                                  \n",
      " reshape_7 (Reshape)         (None, 1, 1, 512)            0         ['dense_15[0][0]']            \n",
      "                                                                                                  \n",
      " multiply_7 (Multiply)       (None, 4, 4, 512)            0         ['leaky_re_lu_14[0][0]',      \n",
      "                                                                     'reshape_7[0][0]']           \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)          (None, 4, 4, 512)            2359808   ['multiply_7[0][0]']          \n",
      "                                                                                                  \n",
      " batch_normalization_16 (Ba  (None, 4, 4, 512)            2048      ['conv2d_19[0][0]']           \n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " leaky_re_lu_15 (LeakyReLU)  (None, 4, 4, 512)            0         ['batch_normalization_16[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " add_7 (Add)                 (None, 4, 4, 512)            0         ['leaky_re_lu_15[0][0]',      \n",
      "                                                                     'add_6[0][0]']               \n",
      "                                                                                                  \n",
      " global_average_pooling2d_8  (None, 512)                  0         ['add_7[0][0]']               \n",
      "  (GlobalAveragePooling2D)                                                                        \n",
      "                                                                                                  \n",
      " dense_16 (Dense)            (None, 512)                  262656    ['global_average_pooling2d_8[0\n",
      "                                                                    ][0]']                        \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 512)                  0         ['dense_16[0][0]']            \n",
      "                                                                                                  \n",
      " dense_17 (Dense)            (None, 64)                   32832     ['dropout[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)         (None, 64)                   0         ['dense_17[0][0]']            \n",
      "                                                                                                  \n",
      " dense_18 (Dense)            (None, 16)                   1040      ['dropout_1[0][0]']           \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)         (None, 16)                   0         ['dense_18[0][0]']            \n",
      "                                                                                                  \n",
      " dense_19 (Dense)            (None, 5)                    85        ['dropout_2[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11573021 (44.15 MB)\n",
      "Trainable params: 11565213 (44.12 MB)\n",
      "Non-trainable params: 7808 (30.50 KB)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, BatchNormalization, LeakyReLU, Add\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Concatenate, Dropout, Reshape, Multiply\n",
    "\n",
    "# Define the FCA block\n",
    "# FCA is used to emphasize important channels and suppress less important ones.\n",
    "def FCA_block(inputs, reduction_ratio=16):\n",
    "    # Get the number of channels in the input tensor\n",
    "    channel_axis = -1\n",
    "    channels = inputs.shape[channel_axis]\n",
    "\n",
    "    # Apply Global Average Pooling to the input tensor to get a channel-wise summary\n",
    "    x = GlobalAveragePooling2D()(inputs)\n",
    "\n",
    "    # Reduce the number of channels by a factor of `reduction_ratio` using a fully connected layer\n",
    "    x = Dense(channels // reduction_ratio, activation='relu')(x)\n",
    "\n",
    "    # Restore the number of channels to the original using another fully connected layer\n",
    "    x = Dense(channels, activation='sigmoid')(x)\n",
    "\n",
    "    # Reshape the output to match the input tensor's shape but with single spatial dimensions\n",
    "    x = Reshape((1, 1, channels))(x)\n",
    "\n",
    "    # Multiply the input tensor with the output of the fully connected layers\n",
    "    # This step modulates the importance of each channel\n",
    "    outputs = Multiply()([inputs, x])\n",
    "    return outputs\n",
    "\n",
    "# Define a residual block with FCA\n",
    "# This block uses residual connections to improve gradient flow and an FCA block for channel attention.\n",
    "def residual_block(inputs, filters, kernel_size=3, strides=1, use_fca=True):\n",
    "    # Apply the first convolutional layer\n",
    "    x = Conv2D(filters, kernel_size, strides=strides, padding='same')(inputs)\n",
    "\n",
    "    # Apply batch normalization to stabilize the training process\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    # Apply the LeakyReLU activation function for non-linearity\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    # Optionally apply the FCA block\n",
    "    if use_fca:\n",
    "        x = FCA_block(x)\n",
    "\n",
    "    # Apply the second convolutional layer\n",
    "    x = Conv2D(filters, kernel_size, strides=1, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU(alpha=0.2)(x)\n",
    "\n",
    "    # Create a shortcut connection that adds the input directly to the output if the dimensions match\n",
    "    if strides != 1 or inputs.shape[-1] != filters:\n",
    "        # If dimensions do not match, apply a convolution to the input to match the output dimensions\n",
    "        shortcut = Conv2D(filters, kernel_size=1, strides=strides, padding='same')(inputs)\n",
    "    else:\n",
    "        # Otherwise, the shortcut is just the input\n",
    "        shortcut = inputs\n",
    "\n",
    "    # Add the shortcut to the output of the second convolutional layer\n",
    "    x = Add()([x, shortcut])\n",
    "    return x\n",
    "\n",
    "# Define the full model\n",
    "def build_model(input_shape=(128, 128, 3), num_classes=5):\n",
    "    inputs = Input(shape=input_shape)  # Define the input layer with the given shape\n",
    "\n",
    "    # Initial convolutional layer to extract basic features\n",
    "    x = Conv2D(64, (7, 7), strides=2, padding='same')(inputs)\n",
    "    x = MaxPooling2D(pool_size=(2, 2))(x)  # Apply max pooling to reduce spatial dimensions\n",
    "    x = BatchNormalization()(x)  # Normalize the activations\n",
    "\n",
    "    # Add residual blocks with increasing complexity and spatial downsampling\n",
    "    x = residual_block(x, 64)  # First block, keeps the spatial dimensions\n",
    "    x = residual_block(x, 64)  # Second block, keeps the spatial dimensions\n",
    "    x = residual_block(x, 128, strides=2)  # Third block, reduces the spatial dimensions by half\n",
    "    x = residual_block(x, 128)  # Fourth block, keeps the spatial dimensions\n",
    "    x = residual_block(x, 256, strides=2)  # Fifth block, reduces the spatial dimensions by half\n",
    "    x = residual_block(x, 256)  # Sixth block, keeps the spatial dimensions\n",
    "    x = residual_block(x, 512, strides=2)  # Seventh block, reduces the spatial dimensions by half\n",
    "    x = residual_block(x, 512)  # Eighth block, keeps the spatial dimensions\n",
    "\n",
    "    # Apply global average pooling to reduce each feature map to a single value\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "\n",
    "    # Fully connected layers for final classification\n",
    "    x = Dense(512, activation='relu')(x)  # Dense layer with 512 units and ReLU activation\n",
    "    x = Dropout(0.5)(x)  # Dropout for regularization\n",
    "    x = Dense(64, activation='relu')(x)  # Dense layer with 64 units and ReLU activation\n",
    "    x = Dropout(0.5)(x)  # Dropout for regularization\n",
    "    x = Dense(16, activation='relu')(x)  # Dense layer with 16 units and ReLU activation\n",
    "    x = Dropout(0.5)(x)  # Dropout for regularization\n",
    "\n",
    "    # Final output layer with softmax activation for classification\n",
    "    outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "    # Create the model\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "# Build and compile the model\n",
    "input_shape = (128, 128, 3)  # Define the input shape of the images\n",
    "num_classes = 5  # Define the number of output classes\n",
    "model = build_model(input_shape, num_classes)\n",
    "\n",
    "# Compile the model with Adam optimizer and sparse categorical crossentropy loss\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary to see the architecture\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined images shape: (112550, 128, 128, 3)\n",
      "Labels shape: (112550,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Paths to the combined images and labels\n",
    "combined_images_path = './combined_images'\n",
    "labels_path = './labels'\n",
    "\n",
    "# Function to load combined images and labels\n",
    "def load_combined_data(record_list):\n",
    "    combined_images = []\n",
    "    labels = []\n",
    "    for record_name in record_list:\n",
    "        combined_images_file = os.path.join(combined_images_path, f'{record_name}_combined_images.npy')\n",
    "        labels_file = os.path.join(labels_path, f'{record_name}_labels.npy')\n",
    "        \n",
    "        combined_images_record = np.load(combined_images_file)\n",
    "        labels_record = np.load(labels_file)\n",
    "        \n",
    "        combined_images.append(combined_images_record)\n",
    "        labels.append(labels_record)\n",
    "    \n",
    "    # Concatenate all records into a single array\n",
    "    combined_images = np.concatenate(combined_images, axis=0)\n",
    "    labels = np.concatenate(labels, axis=0)\n",
    "    \n",
    "    return combined_images, labels\n",
    "\n",
    "# List of records to process\n",
    "record_list = ['100', '101', '102', '103', '104', '105', '106', '107', '108', '109',\n",
    "               '111', '112', '113', '114', '115', '116', '117', '118', '119', '121',\n",
    "               '122', '123', '124', '200', '201', '202', '203', '205', '207', '208',\n",
    "               '209', '210', '212', '213', '214', '215', '217', '219', '220', '221',\n",
    "               '222', '223', '228', '230', '231', '232', '233', '234']\n",
    "\n",
    "# Load the data\n",
    "images, labels = load_combined_data(record_list)\n",
    "\n",
    "# Check the shapes of the loaded data\n",
    "print(f'Combined images shape: {images.shape}')\n",
    "print(f'Labels shape: {labels.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing record: 100\n",
      "Label counts for record 100: Counter({'Normal': 2239, 'SVEB': 33, 'Unknown': 1, 'VEB': 1})\n",
      "Processing record: 101\n",
      "Label counts for record 101: Counter({'Normal': 1860, 'Unknown': 11, 'SVEB': 3})\n",
      "Processing record: 103\n",
      "Label counts for record 103: Counter({'Normal': 2082, 'Unknown': 7, 'SVEB': 2})\n",
      "Processing record: 105\n",
      "Label counts for record 105: Counter({'Normal': 2526, 'Unknown': 124, 'VEB': 41})\n",
      "Processing record: 106\n",
      "Label counts for record 106: Counter({'Normal': 1507, 'VEB': 520, 'Unknown': 71})\n",
      "Processing record: 108\n",
      "Label counts for record 108: Counter({'Normal': 1740, 'Unknown': 61, 'VEB': 17, 'SVEB': 4, 'Fusion': 2})\n",
      "Processing record: 109\n",
      "Label counts for record 109: Counter({'Normal': 2492, 'VEB': 38, 'Unknown': 3, 'Fusion': 2})\n",
      "Processing record: 111\n",
      "Label counts for record 111: Counter({'Normal': 2123, 'Unknown': 9, 'VEB': 1})\n",
      "Processing record: 112\n",
      "Label counts for record 112: Counter({'Normal': 2537, 'Unknown': 11, 'SVEB': 2})\n",
      "Processing record: 113\n",
      "Label counts for record 113: Counter({'Normal': 1789, 'SVEB': 6, 'Unknown': 1})\n",
      "Processing record: 114\n",
      "Label counts for record 114: Counter({'Normal': 1820, 'VEB': 43, 'SVEB': 12, 'Unknown': 11, 'Fusion': 4})\n",
      "Processing record: 115\n",
      "Label counts for record 115: Counter({'Normal': 1953, 'Unknown': 9})\n",
      "Processing record: 116\n",
      "Label counts for record 116: Counter({'Normal': 2302, 'VEB': 109, 'Unknown': 9, 'SVEB': 1})\n",
      "Processing record: 117\n",
      "Label counts for record 117: Counter({'Normal': 1534, 'Unknown': 4, 'SVEB': 1})\n",
      "Processing record: 118\n",
      "Label counts for record 118: Counter({'Normal': 2166, 'SVEB': 96, 'Unknown': 23, 'VEB': 16})\n",
      "Processing record: 119\n",
      "Label counts for record 119: Counter({'Normal': 1543, 'VEB': 444, 'Unknown': 107})\n",
      "Processing record: 121\n",
      "Label counts for record 121: Counter({'Normal': 1861, 'Unknown': 13, 'SVEB': 1, 'VEB': 1})\n",
      "Processing record: 122\n",
      "Label counts for record 122: Counter({'Normal': 2476, 'Unknown': 3})\n",
      "Processing record: 123\n",
      "Label counts for record 123: Counter({'Normal': 1515, 'VEB': 3, 'Unknown': 1})\n",
      "Processing record: 124\n",
      "Label counts for record 124: Counter({'Normal': 1536, 'VEB': 47, 'SVEB': 31, 'Unknown': 15, 'Fusion': 5})\n",
      "Processing record: 200\n",
      "Label counts for record 200: Counter({'Normal': 1743, 'VEB': 826, 'Unknown': 191, 'SVEB': 30, 'Fusion': 2})\n",
      "Processing record: 201\n",
      "Label counts for record 201: Counter({'Normal': 1635, 'VEB': 198, 'SVEB': 128, 'Unknown': 76, 'Fusion': 2})\n",
      "Processing record: 202\n",
      "Label counts for record 202: Counter({'Normal': 2061, 'SVEB': 55, 'VEB': 19, 'Unknown': 10, 'Fusion': 1})\n",
      "Processing record: 203\n",
      "Label counts for record 203: Counter({'Normal': 2529, 'VEB': 444, 'Unknown': 132, 'SVEB': 2, 'Fusion': 1})\n",
      "Processing record: 205\n",
      "Label counts for record 205: Counter({'Normal': 2571, 'VEB': 71, 'Unknown': 16, 'Fusion': 11, 'SVEB': 3})\n",
      "Processing record: 207\n",
      "Label counts for record 207: Counter({'Normal': 1543, 'Unknown': 525, 'VEB': 210, 'SVEB': 107})\n",
      "Processing record: 208\n",
      "Label counts for record 208: Counter({'Normal': 1586, 'VEB': 992, 'Fusion': 373, 'Unknown': 87, 'SVEB': 2})\n",
      "Processing record: 209\n",
      "Label counts for record 209: Counter({'Normal': 2621, 'SVEB': 383, 'Unknown': 47, 'VEB': 1})\n",
      "Processing record: 210\n",
      "Label counts for record 210: Counter({'Normal': 2423, 'VEB': 195, 'Unknown': 35, 'SVEB': 22, 'Fusion': 10})\n",
      "Processing record: 212\n",
      "Label counts for record 212: Counter({'Normal': 2748, 'Unknown': 15})\n",
      "Processing record: 213\n",
      "Label counts for record 213: Counter({'Normal': 2641, 'Fusion': 362, 'VEB': 220, 'Unknown': 43, 'SVEB': 28})\n",
      "Processing record: 214\n",
      "Label counts for record 214: Counter({'Normal': 2003, 'VEB': 256, 'Unknown': 37, 'Fusion': 1})\n",
      "Processing record: 215\n",
      "Label counts for record 215: Counter({'Normal': 3195, 'VEB': 164, 'Unknown': 37, 'SVEB': 3, 'Fusion': 1})\n",
      "Processing record: 219\n",
      "Label counts for record 219: Counter({'Normal': 2082, 'Unknown': 158, 'VEB': 64, 'SVEB': 7, 'Fusion': 1})\n",
      "Processing record: 220\n",
      "Label counts for record 220: Counter({'Normal': 1954, 'SVEB': 94, 'Unknown': 21})\n",
      "Processing record: 221\n",
      "Label counts for record 221: Counter({'Normal': 2031, 'VEB': 396, 'Unknown': 35})\n",
      "Processing record: 222\n",
      "Label counts for record 222: Counter({'Normal': 2274, 'SVEB': 209, 'Unknown': 151})\n",
      "Processing record: 223\n",
      "Label counts for record 223: Counter({'Normal': 2045, 'VEB': 473, 'SVEB': 73, 'Unknown': 38, 'Fusion': 14})\n",
      "Processing record: 228\n",
      "Label counts for record 228: Counter({'Normal': 1688, 'VEB': 362, 'Unknown': 88, 'SVEB': 3})\n",
      "Processing record: 230\n",
      "Label counts for record 230: Counter({'Normal': 2255, 'Unknown': 210, 'VEB': 1})\n",
      "Processing record: 231\n",
      "Label counts for record 231: Counter({'Normal': 1568, 'Unknown': 440, 'VEB': 2, 'SVEB': 1})\n",
      "Processing record: 232\n",
      "Label counts for record 232: Counter({'SVEB': 1382, 'Normal': 398, 'Unknown': 36})\n",
      "Processing record: 233\n",
      "Label counts for record 233: Counter({'Normal': 2230, 'VEB': 831, 'Unknown': 73, 'Fusion': 11, 'SVEB': 7})\n",
      "Processing record: 234\n",
      "Label counts for record 234: Counter({'Normal': 2700, 'SVEB': 50, 'Unknown': 11, 'VEB': 3})\n",
      "Total label counts after exclusion: Counter({'Normal': 90125, 'VEB': 7009, 'Unknown': 3006, 'SVEB': 2781, 'Fusion': 803})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import wfdb\n",
    "from collections import Counter\n",
    "\n",
    "# Define paths\n",
    "data_path = '/home/researchgroup/mahjabeen_workspace/research/CLINet-ECG-Classification-2024/data/mit-bih/mitbih_database/' # Update this path to your data location\n",
    "\n",
    "# List of records to process, excluding those with pacemakers\n",
    "record_list = ['100', '101', '103', '105', '106', '108', '109', '111', '112', '113', \n",
    "               '114', '115', '116', '117', '118', '119', '121', '122', '123', '124', \n",
    "               '200', '201', '202', '203', '205', '207', '208', '209', '210', '212', \n",
    "               '213', '214', '215', '219', '220', '221', '222', '223', '228', '230', \n",
    "               '231', '232', '233', '234']  # Excluding 102, 104, 107, 217\n",
    "\n",
    "# Define the AAMI class mapping\n",
    "aami_mapping = {\n",
    "    'N': 'Normal',\n",
    "    'L': 'Normal',\n",
    "    'R': 'Normal',\n",
    "    'e': 'Normal',\n",
    "    'j': 'Normal',\n",
    "    'A': 'SVEB',\n",
    "    'a': 'SVEB',\n",
    "    'J': 'SVEB',\n",
    "    'S': 'SVEB',\n",
    "    'V': 'VEB',\n",
    "    'E': 'VEB',\n",
    "    'F': 'Fusion',\n",
    "    '/': 'Unknown',\n",
    "    'f': 'Unknown',\n",
    "    'Q': 'Unknown'\n",
    "}\n",
    "\n",
    "# Function to count labels in each record\n",
    "def count_labels_in_record(record_name):\n",
    "    try:\n",
    "        annotation = wfdb.rdann(os.path.join(data_path, record_name), 'atr')\n",
    "        labels = annotation.symbol  # Annotation symbols (labels)\n",
    "        aami_labels = [aami_mapping.get(label, 'Unknown') for label in labels]\n",
    "        return Counter(aami_labels)\n",
    "    except Exception as e:\n",
    "        print(f'Error processing record {record_name}: {e}')\n",
    "        return None\n",
    "\n",
    "# Process each record and display the counts\n",
    "total_labels = Counter()\n",
    "for record_name in record_list:\n",
    "    print(f'Processing record: {record_name}')\n",
    "    label_counts = count_labels_in_record(record_name)\n",
    "    if label_counts:\n",
    "        print(f'Label counts for record {record_name}: {label_counts}')\n",
    "        total_labels.update(label_counts)\n",
    "\n",
    "print(f'Total label counts after exclusion: {total_labels}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: './combined_images'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m combined_images \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./combined_images\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./labels\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/newtest/lib/python3.11/site-packages/numpy/lib/npyio.py:427\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 427\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28mopen\u001b[39m(os_fspath(file), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    428\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: './combined_images'"
     ]
    }
   ],
   "source": [
    "combined_images = np.load('./combined_images')\n",
    "labels = np.load('./labels')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 217_combined_images.npy with shape: (2279, 128, 128, 3)\n",
      "Loaded 122_combined_images.npy with shape: (2476, 128, 128, 3)\n",
      "Loaded 107_combined_images.npy with shape: (2138, 128, 128, 3)\n",
      "Loaded 105_combined_images.npy with shape: (2690, 128, 128, 3)\n",
      "Loaded 106_combined_images.npy with shape: (2097, 128, 128, 3)\n",
      "Loaded 101_combined_images.npy with shape: (1872, 128, 128, 3)\n",
      "Loaded 212_combined_images.npy with shape: (2761, 128, 128, 3)\n",
      "Loaded 220_combined_images.npy with shape: (2066, 128, 128, 3)\n",
      "Loaded 116_combined_images.npy with shape: (2420, 128, 128, 3)\n",
      "Loaded 208_combined_images.npy with shape: (3037, 128, 128, 3)\n",
      "Loaded 108_combined_images.npy with shape: (1822, 128, 128, 3)\n",
      "Loaded 200_combined_images.npy with shape: (2790, 128, 128, 3)\n",
      "Loaded 210_combined_images.npy with shape: (2682, 128, 128, 3)\n",
      "Loaded 223_combined_images.npy with shape: (2641, 128, 128, 3)\n",
      "Loaded 115_combined_images.npy with shape: (1960, 128, 128, 3)\n",
      "Loaded 234_combined_images.npy with shape: (2762, 128, 128, 3)\n",
      "Loaded 202_combined_images.npy with shape: (2145, 128, 128, 3)\n",
      "Loaded 233_combined_images.npy with shape: (3149, 128, 128, 3)\n",
      "Loaded 114_combined_images.npy with shape: (1889, 128, 128, 3)\n",
      "Loaded 104_combined_images.npy with shape: (2308, 128, 128, 3)\n",
      "Loaded 215_combined_images.npy with shape: (3397, 128, 128, 3)\n",
      "Loaded 213_combined_images.npy with shape: (3291, 128, 128, 3)\n",
      "Loaded 121_combined_images.npy with shape: (1874, 128, 128, 3)\n",
      "Loaded 103_combined_images.npy with shape: (2089, 128, 128, 3)\n",
      "Loaded 117_combined_images.npy with shape: (1537, 128, 128, 3)\n",
      "Loaded 118_combined_images.npy with shape: (2299, 128, 128, 3)\n",
      "Loaded 228_combined_images.npy with shape: (2140, 128, 128, 3)\n",
      "Loaded 221_combined_images.npy with shape: (2461, 128, 128, 3)\n",
      "Loaded 109_combined_images.npy with shape: (2532, 128, 128, 3)\n",
      "Loaded 203_combined_images.npy with shape: (3105, 128, 128, 3)\n",
      "Loaded 222_combined_images.npy with shape: (2631, 128, 128, 3)\n",
      "Loaded 112_combined_images.npy with shape: (2547, 128, 128, 3)\n",
      "Loaded 207_combined_images.npy with shape: (2383, 128, 128, 3)\n",
      "Loaded 231_combined_images.npy with shape: (2009, 128, 128, 3)\n",
      "Loaded 232_combined_images.npy with shape: (1815, 128, 128, 3)\n",
      "Loaded 113_combined_images.npy with shape: (1794, 128, 128, 3)\n",
      "Loaded 219_combined_images.npy with shape: (2312, 128, 128, 3)\n",
      "Loaded 124_combined_images.npy with shape: (1632, 128, 128, 3)\n",
      "Loaded 100_combined_images.npy with shape: (2271, 128, 128, 3)\n",
      "Loaded 209_combined_images.npy with shape: (3050, 128, 128, 3)\n",
      "Loaded 205_combined_images.npy with shape: (2670, 128, 128, 3)\n",
      "Loaded 230_combined_images.npy with shape: (2464, 128, 128, 3)\n",
      "Loaded 111_combined_images.npy with shape: (2132, 128, 128, 3)\n",
      "Loaded 102_combined_images.npy with shape: (2189, 128, 128, 3)\n",
      "Loaded 214_combined_images.npy with shape: (2294, 128, 128, 3)\n",
      "Loaded 201_combined_images.npy with shape: (2038, 128, 128, 3)\n",
      "Loaded 119_combined_images.npy with shape: (2093, 128, 128, 3)\n",
      "Loaded 123_combined_images.npy with shape: (1517, 128, 128, 3)\n",
      "Loaded 118_labels.npy with shape: (2299,)\n",
      "Loaded 205_labels.npy with shape: (2670,)\n",
      "Loaded 116_labels.npy with shape: (2420,)\n",
      "Loaded 222_labels.npy with shape: (2631,)\n",
      "Loaded 231_labels.npy with shape: (2009,)\n",
      "Loaded 207_labels.npy with shape: (2383,)\n",
      "Loaded 114_labels.npy with shape: (1889,)\n",
      "Loaded 105_labels.npy with shape: (2690,)\n",
      "Loaded 210_labels.npy with shape: (2682,)\n",
      "Loaded 208_labels.npy with shape: (3037,)\n",
      "Loaded 232_labels.npy with shape: (1815,)\n",
      "Loaded 103_labels.npy with shape: (2089,)\n",
      "Loaded 104_labels.npy with shape: (2308,)\n",
      "Loaded 100_labels.npy with shape: (2271,)\n",
      "Loaded 109_labels.npy with shape: (2532,)\n",
      "Loaded 213_labels.npy with shape: (3291,)\n",
      "Loaded 221_labels.npy with shape: (2461,)\n",
      "Loaded 215_labels.npy with shape: (3397,)\n",
      "Loaded 119_labels.npy with shape: (2093,)\n",
      "Loaded 209_labels.npy with shape: (3050,)\n",
      "Loaded 233_labels.npy with shape: (3149,)\n",
      "Loaded 219_labels.npy with shape: (2312,)\n",
      "Loaded 217_labels.npy with shape: (2279,)\n",
      "Loaded 202_labels.npy with shape: (2145,)\n",
      "Loaded 234_labels.npy with shape: (2762,)\n",
      "Loaded 106_labels.npy with shape: (2097,)\n",
      "Loaded 220_labels.npy with shape: (2066,)\n",
      "Loaded 200_labels.npy with shape: (2790,)\n",
      "Loaded 111_labels.npy with shape: (2132,)\n",
      "Loaded 107_labels.npy with shape: (2138,)\n",
      "Loaded 115_labels.npy with shape: (1960,)\n",
      "Loaded 230_labels.npy with shape: (2464,)\n",
      "Loaded 117_labels.npy with shape: (1537,)\n",
      "Loaded 214_labels.npy with shape: (2294,)\n",
      "Loaded 108_labels.npy with shape: (1822,)\n",
      "Loaded 102_labels.npy with shape: (2189,)\n",
      "Loaded 101_labels.npy with shape: (1872,)\n",
      "Loaded 113_labels.npy with shape: (1794,)\n",
      "Loaded 203_labels.npy with shape: (3105,)\n",
      "Loaded 212_labels.npy with shape: (2761,)\n",
      "Loaded 223_labels.npy with shape: (2641,)\n",
      "Loaded 123_labels.npy with shape: (1517,)\n",
      "Loaded 228_labels.npy with shape: (2140,)\n",
      "Loaded 124_labels.npy with shape: (1632,)\n",
      "Loaded 121_labels.npy with shape: (1874,)\n",
      "Loaded 201_labels.npy with shape: (2038,)\n",
      "Loaded 122_labels.npy with shape: (2476,)\n",
      "Loaded 112_labels.npy with shape: (2547,)\n",
      "Combined images shape: (112550, 128, 128, 3)\n",
      "Labels shape: (112550,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define the directories containing the .npy files\n",
    "combined_images_dir = './combined_images'\n",
    "labels_dir = './labels'\n",
    "\n",
    "# Initialize lists to store the loaded data\n",
    "all_combined_images = []\n",
    "all_labels = []\n",
    "\n",
    "# Load combined images from each .npy file in the directory\n",
    "for filename in os.listdir(combined_images_dir):\n",
    "    if filename.endswith('.npy'):\n",
    "        file_path = os.path.join(combined_images_dir, filename)\n",
    "        images = np.load(file_path)\n",
    "        all_combined_images.append(images)\n",
    "        print(f'Loaded {filename} with shape: {images.shape}')\n",
    "\n",
    "# Load labels from each .npy file in the directory\n",
    "for filename in os.listdir(labels_dir):\n",
    "    if filename.endswith('.npy'):\n",
    "        file_path = os.path.join(labels_dir, filename)\n",
    "        labels = np.load(file_path)\n",
    "        all_labels.append(labels)\n",
    "        print(f'Loaded {filename} with shape: {labels.shape}')\n",
    "\n",
    "# Concatenate all images and labels into single arrays\n",
    "combined_images = np.concatenate(all_combined_images, axis=0)\n",
    "labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "print(f'Combined images shape: {combined_images.shape}')\n",
    "print(f'Labels shape: {labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(combined_images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the training data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "\n",
    "# Print shapes\n",
    "print(f'Training data shape: {X_train.shape}, {y_train.shape}')\n",
    "print(f'Validation data shape: {X_val.shape}, {y_val.shape}')\n",
    "print(f'Testing data shape: {X_test.shape}, {y_test.shape}')\n",
    "\n",
    "# Define paths for saving the data\n",
    "save_dir = './split_data'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save the training, validation, and test sets\n",
    "np.save(os.path.join(save_dir, 'X_train.npy'), X_train)\n",
    "np.save(os.path.join(save_dir, 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(save_dir, 'X_val.npy'), X_val)\n",
    "np.save(os.path.join(save_dir, 'y_val.npy'), y_val)\n",
    "np.save(os.path.join(save_dir, 'X_test.npy'), X_test)\n",
    "np.save(os.path.join(save_dir, 'y_test.npy'), y_test)\n",
    "\n",
    "print(\"Data has been saved successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels file for 217_combined_images.npy not found.\n",
      "Labels file for 122_combined_images.npy not found.\n",
      "Labels file for 107_combined_images.npy not found.\n",
      "Labels file for 105_combined_images.npy not found.\n",
      "Labels file for 106_combined_images.npy not found.\n",
      "Labels file for 101_combined_images.npy not found.\n",
      "Labels file for 212_combined_images.npy not found.\n",
      "Labels file for 220_combined_images.npy not found.\n",
      "Labels file for 116_combined_images.npy not found.\n",
      "Labels file for 208_combined_images.npy not found.\n",
      "Labels file for 108_combined_images.npy not found.\n",
      "Labels file for 200_combined_images.npy not found.\n",
      "Labels file for 210_combined_images.npy not found.\n",
      "Labels file for 223_combined_images.npy not found.\n",
      "Labels file for 115_combined_images.npy not found.\n",
      "Labels file for 234_combined_images.npy not found.\n",
      "Labels file for 202_combined_images.npy not found.\n",
      "Labels file for 233_combined_images.npy not found.\n",
      "Labels file for 114_combined_images.npy not found.\n",
      "Labels file for 104_combined_images.npy not found.\n",
      "Labels file for 215_combined_images.npy not found.\n",
      "Labels file for 213_combined_images.npy not found.\n",
      "Labels file for 121_combined_images.npy not found.\n",
      "Labels file for 103_combined_images.npy not found.\n",
      "Labels file for 117_combined_images.npy not found.\n",
      "Labels file for 118_combined_images.npy not found.\n",
      "Labels file for 228_combined_images.npy not found.\n",
      "Labels file for 221_combined_images.npy not found.\n",
      "Labels file for 109_combined_images.npy not found.\n",
      "Labels file for 203_combined_images.npy not found.\n",
      "Labels file for 222_combined_images.npy not found.\n",
      "Labels file for 112_combined_images.npy not found.\n",
      "Labels file for 207_combined_images.npy not found.\n",
      "Labels file for 231_combined_images.npy not found.\n",
      "Labels file for 232_combined_images.npy not found.\n",
      "Labels file for 113_combined_images.npy not found.\n",
      "Labels file for 219_combined_images.npy not found.\n",
      "Labels file for 124_combined_images.npy not found.\n",
      "Labels file for 100_combined_images.npy not found.\n",
      "Labels file for 209_combined_images.npy not found.\n",
      "Labels file for 205_combined_images.npy not found.\n",
      "Labels file for 230_combined_images.npy not found.\n",
      "Labels file for 111_combined_images.npy not found.\n",
      "Labels file for 102_combined_images.npy not found.\n",
      "Labels file for 214_combined_images.npy not found.\n",
      "Labels file for 201_combined_images.npy not found.\n",
      "Labels file for 119_combined_images.npy not found.\n",
      "Labels file for 123_combined_images.npy not found.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Load data in batches\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m combined_images, labels \u001b[38;5;241m=\u001b[39m \u001b[43mload_data_in_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombined_images_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCombined images shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcombined_images\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLabels shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 42\u001b[0m, in \u001b[0;36mload_data_in_batches\u001b[0;34m(batch_size, combined_images_dir, labels_dir)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessed batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_batches\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Concatenate all batches into single arrays\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m combined_images \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_combined_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m labels \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(all_labels, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m combined_images, labels\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "#After crash, reloading into smaller batches.\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define the directories containing the .npy files\n",
    "combined_images_dir = './combined_images'\n",
    "labels_dir = './labels'\n",
    "\n",
    "# Function to load data in batches\n",
    "def load_data_in_batches(batch_size, combined_images_dir, labels_dir):\n",
    "    all_combined_images = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Load combined images and labels from each .npy file in batches\n",
    "    for filename in os.listdir(combined_images_dir):\n",
    "        if filename.endswith('.npy'):\n",
    "            images_path = os.path.join(combined_images_dir, filename)\n",
    "            labels_path = os.path.join(labels_dir, filename)\n",
    "            \n",
    "            if not os.path.exists(labels_path):\n",
    "                print(f'Labels file for {filename} not found.')\n",
    "                continue\n",
    "            \n",
    "            images = np.load(images_path)\n",
    "            labels = np.load(labels_path)\n",
    "            \n",
    "            print(f'Loaded {filename} with image shape: {images.shape} and label shape: {labels.shape}')\n",
    "            \n",
    "            # Determine the number of batches\n",
    "            num_batches = len(images) // batch_size + (1 if len(images) % batch_size != 0 else 0)\n",
    "            \n",
    "            for i in range(num_batches):\n",
    "                batch_images = images[i * batch_size:(i + 1) * batch_size]\n",
    "                batch_labels = labels[i * batch_size:(i + 1) * batch_size]\n",
    "                \n",
    "                all_combined_images.append(batch_images)\n",
    "                all_labels.append(batch_labels)\n",
    "                \n",
    "                print(f'Processed batch {i+1}/{num_batches} for {filename}')\n",
    "    \n",
    "    # Concatenate all batches into single arrays\n",
    "    combined_images = np.concatenate(all_combined_images, axis=0)\n",
    "    labels = np.concatenate(all_labels, axis=0)\n",
    "    \n",
    "    return combined_images, labels\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 1000\n",
    "\n",
    "# Load data in batches\n",
    "combined_images, labels = load_data_in_batches(batch_size, combined_images_dir, labels_dir)\n",
    "\n",
    "print(f'Combined images shape: {combined_images.shape}')\n",
    "print(f'Labels shape: {labels.shape}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels file for 217_combined_images.npy not found.\n",
      "Labels file for 122_combined_images.npy not found.\n",
      "Labels file for 107_combined_images.npy not found.\n",
      "Labels file for 105_combined_images.npy not found.\n",
      "Labels file for 106_combined_images.npy not found.\n",
      "Labels file for 101_combined_images.npy not found.\n",
      "Labels file for 212_combined_images.npy not found.\n",
      "Labels file for 220_combined_images.npy not found.\n",
      "Labels file for 116_combined_images.npy not found.\n",
      "Labels file for 208_combined_images.npy not found.\n",
      "Labels file for 108_combined_images.npy not found.\n",
      "Labels file for 200_combined_images.npy not found.\n",
      "Labels file for 210_combined_images.npy not found.\n",
      "Labels file for 223_combined_images.npy not found.\n",
      "Labels file for 115_combined_images.npy not found.\n",
      "Labels file for 234_combined_images.npy not found.\n",
      "Labels file for 202_combined_images.npy not found.\n",
      "Labels file for 233_combined_images.npy not found.\n",
      "Labels file for 114_combined_images.npy not found.\n",
      "Labels file for 104_combined_images.npy not found.\n",
      "Labels file for 215_combined_images.npy not found.\n",
      "Labels file for 213_combined_images.npy not found.\n",
      "Labels file for 121_combined_images.npy not found.\n",
      "Labels file for 103_combined_images.npy not found.\n",
      "Labels file for 117_combined_images.npy not found.\n",
      "Labels file for 118_combined_images.npy not found.\n",
      "Labels file for 228_combined_images.npy not found.\n",
      "Labels file for 221_combined_images.npy not found.\n",
      "Labels file for 109_combined_images.npy not found.\n",
      "Labels file for 203_combined_images.npy not found.\n",
      "Labels file for 222_combined_images.npy not found.\n",
      "Labels file for 112_combined_images.npy not found.\n",
      "Labels file for 207_combined_images.npy not found.\n",
      "Labels file for 231_combined_images.npy not found.\n",
      "Labels file for 232_combined_images.npy not found.\n",
      "Labels file for 113_combined_images.npy not found.\n",
      "Labels file for 219_combined_images.npy not found.\n",
      "Labels file for 124_combined_images.npy not found.\n",
      "Labels file for 100_combined_images.npy not found.\n",
      "Labels file for 209_combined_images.npy not found.\n",
      "Labels file for 205_combined_images.npy not found.\n",
      "Labels file for 230_combined_images.npy not found.\n",
      "Labels file for 111_combined_images.npy not found.\n",
      "Labels file for 102_combined_images.npy not found.\n",
      "Labels file for 214_combined_images.npy not found.\n",
      "Labels file for 201_combined_images.npy not found.\n",
      "Labels file for 119_combined_images.npy not found.\n",
      "Labels file for 123_combined_images.npy not found.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No data was loaded. Please check the filenames and paths.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 57\u001b[0m\n\u001b[1;32m     54\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# Load data in batches\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m combined_images, labels, missing_files \u001b[38;5;241m=\u001b[39m \u001b[43mload_data_in_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcombined_images_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Log missing files\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_files:\n",
      "Cell \u001b[0;32mIn[5], line 45\u001b[0m, in \u001b[0;36mload_data_in_batches\u001b[0;34m(batch_size, combined_images_dir, labels_dir)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Check if any data was loaded\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m all_combined_images \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m all_labels:\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo data was loaded. Please check the filenames and paths.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Concatenate all batches into single arrays\u001b[39;00m\n\u001b[1;32m     48\u001b[0m combined_images \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate(all_combined_images, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: No data was loaded. Please check the filenames and paths."
     ]
    }
   ],
   "source": [
    "#debug\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Define the directories containing the .npy files\n",
    "combined_images_dir = './combined_images'\n",
    "labels_dir = './labels'\n",
    "\n",
    "# Function to load data in batches\n",
    "def load_data_in_batches(batch_size, combined_images_dir, labels_dir):\n",
    "    all_combined_images = []\n",
    "    all_labels = []\n",
    "    missing_files = []\n",
    "\n",
    "    # Load combined images and labels from each .npy file in batches\n",
    "    for filename in os.listdir(combined_images_dir):\n",
    "        if filename.endswith('.npy'):\n",
    "            images_path = os.path.join(combined_images_dir, filename)\n",
    "            labels_path = os.path.join(labels_dir, filename)\n",
    "            \n",
    "            if not os.path.exists(labels_path):\n",
    "                print(f'Labels file for {filename} not found.')\n",
    "                missing_files.append(filename)\n",
    "                continue\n",
    "            \n",
    "            images = np.load(images_path)\n",
    "            labels = np.load(labels_path)\n",
    "            \n",
    "            print(f'Loaded {filename} with image shape: {images.shape} and label shape: {labels.shape}')\n",
    "            \n",
    "            # Determine the number of batches\n",
    "            num_batches = len(images) // batch_size + (1 if len(images) % batch_size != 0 else 0)\n",
    "            \n",
    "            for i in range(num_batches):\n",
    "                batch_images = images[i * batch_size:(i + 1) * batch_size]\n",
    "                batch_labels = labels[i * batch_size:(i + 1) * batch_size]\n",
    "                \n",
    "                all_combined_images.append(batch_images)\n",
    "                all_labels.append(batch_labels)\n",
    "                \n",
    "                print(f'Processed batch {i+1}/{num_batches} for {filename}')\n",
    "    \n",
    "    # Check if any data was loaded\n",
    "    if not all_combined_images or not all_labels:\n",
    "        raise ValueError(\"No data was loaded. Please check the filenames and paths.\")\n",
    "    \n",
    "    # Concatenate all batches into single arrays\n",
    "    combined_images = np.concatenate(all_combined_images, axis=0)\n",
    "    labels = np.concatenate(all_labels, axis=0)\n",
    "    \n",
    "    return combined_images, labels, missing_files\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 1000\n",
    "\n",
    "# Load data in batches\n",
    "combined_images, labels, missing_files = load_data_in_batches(batch_size, combined_images_dir, labels_dir)\n",
    "\n",
    "# Log missing files\n",
    "if missing_files:\n",
    "    print(\"Missing files:\")\n",
    "    for file in missing_files:\n",
    "        print(file)\n",
    "else:\n",
    "    print(\"All files matched successfully.\")\n",
    "\n",
    "print(f'Combined images shape: {combined_images.shape}')\n",
    "print(f'Labels shape: {labels.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newtest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
